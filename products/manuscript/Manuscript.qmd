---
title: "Final project"
author: "Murtaza Yaqubi"
format: html
editor: visual
---

## General Background Information

Coronary artery disease (CAD), also known as coronary heart disease or ischemic heart disease. It’s also what most people mean when they use the general term “heart disease”. It is a common and serious condition where plaque buildup in the coronary arteries restricts blood flow to the heart. This gradual narrowing, caused by atherosclerosis, often develops over years without symptoms until a heart attack occurs, making CAD a "silent killer." Symptoms, when present, include chest pain and shortness of breath. As the leading cause of death in the U.S. and globally, CAD claimed 375,500 lives in the U.S. in 2021 and affects over 18 million American adults.

## Description of data and data source

This dataset contains anonymized patient records related to cardiovascular disease (CVD) risk factors and diagnoses. Each row represents a patient, with features like age, gender, blood pressure, cholesterol levels, and clinical indicators (e.g., ST depression, chest pain type).

This heart disease dataset is acquired from one o f the multispecialty hospitals in India. This dataset consists of 1000 subjects so each row represents a patient, with features like age, gender, blood pressure, cholesterol levels, and clinical indicators (e.g., ST depression, chest pain type). This dataset was obtained from mendeley data where it was first published in April of 2021.

This dataset can be accessed at <https://data.mendeley.com/datasets/dzz48mvjht/1>

## Questions/Hypotheses to be addressed

Do individuals younger than 50 with normal cholesterol levels (within "healthy" range) still face CVD risk if they have elevated blood pressure or ST depression (oldpeak)?

refernece: 1- https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings 1- https://www.mayoclinic.org/diseases-conditions/prehypertension/symptoms-causes/syc-20376703 2- https://my.clevelandclinic.org/health/articles/11920-cholesterol-numbers-what-do-they-mean 2- https://www.hopkinsmedicine.org/health/treatment-tests-and-therapies/lipid-panel 3- https://pmc.ncbi.nlm.nih.gov/articles/PMC6376358/ 3- https://www.ahajournals.org/doi/pdf/10.1161/01.STR.25.9.1820

## Methods

In this study, we will investigate whether younger individuals with clinically normal serum cholesterol levels are at an increased risk of cardiovascular disease (CVD) when additional risk factors—specifically elevated blood pressure and ST depression—are present. The analysis will be based on a cardiovascular disease dataset where the primary outcome, “target,” indicates the absence or presence of heart disease, and key predictors include age, serum cholesterol, resting blood pressure, and ST depression (oldpeak).

We will begin by restricting our study population to patients under 50 years of age with serum cholesterol levels below 200 mg/dL, a threshold that reflects clinically normal values. Within this subset, patients will be categorized according to secondary risk factors: those with a resting blood pressure between 120 and 129 mmHg will be designated as having elevated blood pressure, and those with an oldpeak value greater than 1 will be classified as exhibiting elevated ST depression. Descriptive analyses will then be performed to compare CVD prevalence across these risk groups, providing initial insights into potential associations between these non-traditional risk factors and CVD in a younger population.

To validate these observations, chi-square tests will be applied to assess the statistical significance of differences in CVD prevalence among the defined groups. Furthermore, logistic regression models will be used to quantify the independent and combined effects of elevated blood pressure and ST depression on CVD risk, with additional adjustments made for potential confounders such as gender and fasting blood sugar. Model performance will be evaluated using outputs such as odds ratios with 95% confidence intervals, relative risks, and receiver operating characteristic (ROC) curves with corresponding area under the curve (AUC) values.

In addition to these inferential techniques, we will develop predictive models employing linear regression, LASSO regression, and random forest methods. These models will be optimized through grid search and cross-validation procedures, and their predictive accuracy will be measured using the root mean squared error (RMSE). By integrating these methodologies, readers can expect a comprehensive evaluation of CVD risk among younger individuals, ultimately challenging conventional risk-assessment frameworks and supporting more nuanced screening approaches for early detection of cardiovascular disease.

## **Conclusion**

Based solely on the outputs obtained from our analysis, there is little evidence to support the hypothesis that younger individuals with normal cholesterol levels face an increased risk of heart disease when secondary risk factors, namely elevated blood pressure and ST depression are present. Chi-square tests revealed no significant differences in the prevalence of cardiovascular disease across the risk groups. In the logistic regression models, the estimated odds ratio for elevated blood pressure was approximately 2.29 (95% CI: 0.55–12.00) and for ST depression about 1.83 (95% CI: 0.07–27.22), though neither effect reached statistical significance. Similarly, the relative risk estimates were around 1.66 (95% CI: 0.73–5.19) for elevated blood pressure and 1.21 (95% CI: 0.61–2.35) for ST depression, further suggesting that these factors do not significantly impact the risk of heart disease in this subgroup. Additionally, predictive models including linear regression, LASSO, and random forest, produced modest performance (with RMSE values near 0.47–0.48 and an AUC of about 0.60), reinforcing the conclusion that these secondary risk factors do not markedly improve prediction of cardiovascular disease. Overall, the findings do not provide strong evidence that the presence of elevated blood pressure or ST depression significantly increases CVD risk among younger individuals with normal cholesterol levels.

## Data aquisition

This heart disease dataset is acquired from one o f the multispecialty hospitals in India and it was first published in April of 2021.

This dataset can be accessed at <https://data.mendeley.com/datasets/dzz48mvjht/1>

# Load necessary libraries

```{r}
library(knitr)
library(readr)
library(dplyr)
library(ggplot2)
library(ggcorrplot)
library(vcd)
library(caret)        # For data partitioning and model training
library(randomForest) # For Random Forest modeling
library(tidyverse)
library(tidymodels)
library(broom)
library(here)           # For file paths
library(pROC)
library(naniar)         # For gg_miss_var()
```

# PART 1: DATA IMPORT AND CLEANING

```{r}
# Import the dataset
cardio_df <- read_csv(here("data", "raw-data", "Cardiovascular_Disease_Dataset.csv"))

# Basic exploration of the dataset
head(cardio_df)           # View the first few rows
colnames(cardio_df)       # Check column names
dim(cardio_df)            # Check dimensions (should be 1025 rows x 14 columns)
summary(cardio_df)        # Get a summary of the data
glimpse(cardio_df)        # Glimpse at the data structure

# Visualize missing values for each variable
gg_miss_var(cardio_df)    # Display number of NAs

# Transform variables into suitable types and add descriptive labels
cardio_df <- cardio_df %>%
  mutate(
    gender = factor(gender, levels = c(0, 1), labels = c("Female", "Male")), 
    chestpain = factor(chestpain, levels = c(0, 1, 2, 3),
                       labels = c("Typical Angina", "Atypical Angina", "Non-anginal Pain", "Asymptomatic")),
    fastingbloodsugar = factor(fastingbloodsugar, levels = c(0, 1), labels = c("False", "True")),
    restingrelectro = factor(restingrelectro, levels = c(0, 1, 2),
                             labels = c("Normal", "ST-T Abnormality", "Left Ventricular Hypertrophy")),
    exerciseangia = factor(exerciseangia, levels = c(0, 1), labels = c("No", "Yes")),
    slope = factor(slope, levels = c(1, 2, 3), labels = c("Upsloping", "Flat", "Downsloping")),
    target = factor(target, levels = c(0, 1), labels = c("No Heart Disease", "Heart Disease"))
  )

# Print the transformed dataset for verification
print(cardio_df)
```

# PART 2: EXPLORATORY DATA ANALYSIS (EDA)

```{r}
# Calculate summary statistics for selected variables
summary_stats <- cardio_df %>% 
  summarise(
    # Age statistics
    Age_min = min(age, na.rm = TRUE),
    Age_max = max(age, na.rm = TRUE),
    Age_mean = mean(age, na.rm = TRUE),
    Age_sd = sd(age, na.rm = TRUE),
    
    # Resting Blood Pressure statistics
    RestingBP_min = min(restingBP, na.rm = TRUE),
    RestingBP_max = max(restingBP, na.rm = TRUE),
    RestingBP_mean = mean(restingBP, na.rm = TRUE),
    RestingBP_sd = sd(restingBP, na.rm = TRUE),
    
    # Serum Cholesterol statistics (excluding zeros)
    SerumCholesterol_min = min(serumcholestrol[serumcholestrol > 0], na.rm = TRUE),
    SerumCholestrol_max = max(serumcholestrol, na.rm = TRUE),
    SerumCholestrol_mean = mean(serumcholestrol, na.rm = TRUE),
    SerumCholestrol_sd = sd(serumcholestrol, na.rm = TRUE),
    
    # Maximum Heart Rate statistics
    MaxHeartRate_min = min(maxheartrate, na.rm = TRUE),
    MaxHeartRate_max = max(maxheartrate, na.rm = TRUE),
    MaxHeartRate_mean = mean(maxheartrate, na.rm = TRUE),
    MaxHeartRate_sd = sd(maxheartrate, na.rm = TRUE),
    
    # Oldpeak statistics (excluding zeros)
    Oldpeak_min = min(oldpeak[oldpeak > 0], na.rm = TRUE),
    Oldpeak_max = max(oldpeak, na.rm = TRUE),
    Oldpeak_mean = mean(oldpeak, na.rm = TRUE),
    Oldpeak_sd = sd(oldpeak, na.rm = TRUE)
  )

# Print summary statistics
print(summary_stats)

# Create histograms and density overlays for key variables

# Age distribution
ggplot(cardio_df, aes(x = age)) +
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "steelblue", alpha = 0.5) +
  geom_density(color = "darkblue", linewidth = 1) +
  labs(title = "Age Distribution", x = "Age", y = "Density") +
  scale_x_continuous(breaks = seq(0, max(cardio_df$age, na.rm = TRUE) + 10, by = 10)) +
  theme_bw()

# Serum Cholesterol distribution (excluding zeros)
ggplot(subset(cardio_df, serumcholestrol > 0), aes(x = serumcholestrol)) +
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "forestgreen", alpha = 0.5) +
  geom_density(color = "darkgreen", linewidth = 1) +
  labs(title = "Serum Cholesterol Distribution", 
       x = "Serum Cholesterol (mg/dL)", y = "Density") +
  theme_minimal()

# Resting Blood Pressure distribution (excluding zeros)
ggplot(subset(cardio_df, restingBP > 0), aes(x = restingBP)) +
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "maroon", alpha = 0.5) +
  geom_density(color = "darkred", linewidth = 1) +
  labs(title = "Resting Blood Pressure Distribution", 
       x = "Resting Blood Pressure (mmHG)", y = "Density") +
  theme_bw()

# Boxplot for resting blood pressure by heart disease status
ggplot(cardio_df, aes(x = target, y = restingBP, fill = target)) +
  geom_boxplot() +
  labs(title = "Resting Blood Pressure by Heart Disease Status",
       x = "Heart Disease Status",
       y = "Resting Blood Pressure (mmHg)") +
  theme_minimal()

# Violin plot for oldpeak by heart disease status
ggplot(cardio_df, aes(x = target, y = oldpeak, fill = target)) +
  geom_violin(trim = FALSE) +
  labs(title = "Oldpeak Distribution by Heart Disease Status",
       x = "Heart Disease Status",
       y = "Oldpeak") +
  theme_minimal()

# Create the correlation matrix for selected numeric variables
numeric_vars <- cardio_df %>% 
  select(age, restingBP, serumcholestrol, maxheartrate, oldpeak)
cor_matrix <- cor(numeric_vars, use = "pairwise.complete.obs")

# Plot the correlation matrix with ggcorrplot
ggcorrplot(cor_matrix, 
           lab = TRUE,                         
           lab_size = 3,                       
           colors = c("blue", "white", "red"),  # Color gradient from blue (negative) to red (positive)
           title = "Correlation Matrix", 
           ggtheme = theme_minimal())       
```

# PART 3: STATISTICAL ANALYSIS: DATA CLEANING, SUMMARIZATION, AND VISUALIZATION

```{r}
# Filter the data for individuals under 50 with normal cholesterol (<200 mg/dL) and create risk indicator variables for elevated blood pressure and ST depression.
cleaned_df <- cardio_df %>%
  filter(age < 50, serumcholestrol > 0, serumcholestrol < 200) %>%  
  mutate(
    # Create a binary variable for blood pressure:
    # "Elevated" if restingBP is between 120 and 129; otherwise, "Otherwise"
    elevated_BP = if_else(between(restingBP, 120, 129), "Elevated", "Otherwise"),
    
    # Create a binary indicator for ST depression:
    # "Elevated" if oldpeak is greater than 1; otherwise, "Normal"
    elevated_oldpeak = if_else(oldpeak > 1, "Elevated", "Normal")
  ) %>% 
  # Select the variables we want to keep
  select(age, gender, chestpain, restingBP, serumcholestrol, fastingbloodsugar, 
         maxheartrate, oldpeak, target, elevated_BP, elevated_oldpeak)

# Print a sample of the cleaned dataset
head(cleaned_df)

# Create a summary table by grouping the cleaned data by blood pressure status, ST depression status, and heart disease (target)
summary_table <- cleaned_df %>%
  group_by(elevated_BP, elevated_oldpeak, target) %>%
  summarise(count = n(), .groups = "drop")
print(summary_table)


# Interaction Visualization: Heatmap
interaction_counts <- cleaned_df %>%
  group_by(elevated_BP, elevated_oldpeak, target) %>%
  summarise(count = n(), .groups = "drop")

heatmap_plot <- ggplot(interaction_counts, aes(x = elevated_BP, y = elevated_oldpeak, fill = count)) +
  geom_tile(color = "white", size = 0.5) +
  geom_text(aes(label = count), color = "black", size = 5) +
  facet_wrap(~ target, ncol = 2) +
  scale_fill_gradient(low = "lightgreen", high = "forestgreen") +
  labs(title = "Interaction between Blood Pressure and ST Depression",
       subtitle = "By Heart Disease Status (Individuals <50 with Normal Cholesterol)",
       x = "Blood Pressure Status",
       y = "ST Depression Status",
       fill = "Count") +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold"),
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )
print(heatmap_plot)

# Visualize CVD risk by blood pressure and ST depression status using faceted grouped Bar Chart
ggplot(interaction_counts, aes(x = elevated_BP, y = count, fill = target)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) +
  facet_wrap(~ elevated_oldpeak, ncol = 2, scales = "free_y") +
  labs(title = "CVD Risk by Blood Pressure and ST Depression Status",
       subtitle = "Individuals Under 50 with Normal Cholesterol (<200 mg/dL)",
       x = "Blood Pressure Status",
       y = "Count",
       fill = "Heart Disease") +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold"),
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "bottom"
  )
```

# PART 4a: INFERENTIAL STATISTICAL ANALYSIS

# Chi-Square Tests

```{r}
# Create a contingency table for blood pressure status vs. heart disease
table_bp_target <- table(cleaned_df$elevated_BP, cleaned_df$target)
print(table_bp_target)
chisq_test_bp <- chisq.test(table_bp_target)
print(chisq_test_bp)

# Create a contingency table for ST depression vs. heart disease
table_oldpeak_target <- table(cleaned_df$elevated_oldpeak, cleaned_df$target)
print(table_oldpeak_target)
chisq_test_oldpeak <- chisq.test(table_oldpeak_target)
print(chisq_test_oldpeak)
```

The chi-square tests show that there is no statistically significant association between the risk factors and heart disease in this subgroup. For blood pressure, the test (χ² = 1.012, p = 0.3144) indicates no significant difference in heart disease rates between those with elevated blood pressure and those with otherwise normal levels. Similarly, for ST depression, the test (χ² = 0.35953, p = 0.5488) suggests that heart disease prevalence does not differ significantly between patients with elevated versus normal ST depression.

# Logistic Regression Analysis

```{r}
# Convert target to a binary outcome for regression (1 = Heart Disease, 0 = No Heart Disease)
cleaned_df <- cleaned_df %>%
  mutate(target_binary = if_else(target == "Heart Disease", 1, 0))

# Fit a logistic regression model with elevated_BP and elevated_oldpeak as predictors
model <- glm(target_binary ~ elevated_BP + elevated_oldpeak, 
             data = cleaned_df, family = binomial)
summary(model)

# Fit a logistic regression model including the interaction term
model_interaction <- glm(target_binary ~ elevated_BP * elevated_oldpeak, 
                         data = cleaned_df, family = binomial)
summary(model_interaction)
```

In the logistic regression model that includes only elevated blood pressure and ST depression as predictors, the intercept is significant (p ≈ 0.034), which tells us that the baseline log odds of heart disease (when both predictors are at their reference levels) differ from zero. However, neither elevated blood pressure (p ≈ 0.259) nor elevated ST depression (p ≈ 0.563) are statistically significant predictors of heart disease on their own. When we include an interaction term between elevated blood pressure and ST depression, the results remain similar: the intercept is still significant (p ≈ 0.046), but the main effects of elevated blood pressure (p ≈ 0.279) and elevated ST depression (p ≈ 0.662), as well as their interaction (p ≈ 0.824), do not reach significance. These findings suggest that, in this analysis, neither elevated blood pressure nor elevated ST depression—nor their combination—provides significant predictive information for the presence of heart disease among these individuals.

# Extract Odds Ratios (OR) from the logistic regression model with interaction

```{r}
or_values <- exp(coef(model_interaction))
or_ci <- exp(confint(model_interaction))
or_results <- data.frame(
  Term = names(or_values),
  OR = or_values,
  CI_lower = or_ci[, 1],
  CI_upper = or_ci[, 2]
)
print("Odds Ratios (with 95% CI):")
print(or_results)


# Tidy the model output and compute odds ratios and 95% CI
tidy_model <- tidy(model_interaction, exponentiate = TRUE, conf.int = TRUE)
print(tidy_model)
```

The odds ratios indicate that when all predictors are at their reference levels, the baseline odds of heart disease are estimated at about 0.27 (95% CI: 0.06–0.87). For patients with elevated blood pressure (compared to the reference), the odds of heart disease are roughly 2.29 times higher (95% CI: 0.55–12.00), while those with normal ST depression have an odds ratio of about 1.83 (95% CI: 0.07–27.22). The interaction term between elevated blood pressure and normal ST depression shows an odds ratio of approximately 0.71 (95% CI: 0.04–21.82). However, the very wide confidence intervals across these estimates suggest a high degree of uncertainty, and none of these effects appear to be statistically significant.

# Fit a log-binomial model to estimate Relative Risks (RR)

```{r}
rr_model <- glm(target_binary ~ elevated_BP + elevated_oldpeak, 
                data = cleaned_df, family = binomial(link = "log"))
rr_values <- exp(coef(rr_model))
rr_ci <- exp(confint(rr_model))
rr_results <- data.frame(
  Term = names(rr_values),
  RR = rr_values,
  CI_lower = rr_ci[, 1],
  CI_upper = rr_ci[, 2]
)
print("Relative Risks (with 95% CI):")
print(rr_results)
```

The baseline relative risk is about 0.23 (95% CI: 0.08–0.46). For patients in the "elevated_BPOtherwise" category, the risk increases to roughly 1.66 (95% CI: 0.73–5.19), and for those in the "elevated_oldpeakNormal" category, the risk is around 1.21 (95% CI: 0.61–2.35). The wide confidence intervals suggest some uncertainty, and since they include 1, these differences may not be statistically significant.

# ROC Curve and AUC for the logistic regression model with interaction

```{r}
roc_curve <- roc(cleaned_df$target_binary, predict(model_interaction, type = "response"))
plot(roc_curve, main = "ROC Curve for Logistic Regression Model", col = "blue", lwd = 2)
auc_value <- auc(roc_curve)
print(paste("AUC:", auc_value))
```

The area under the ROC curve is about 0.60, suggesting that this logistic regression model only slightly outperforms random guessing. It provides modest discrimination between individuals with and without heart disease in this subset, but is not particularly strong.

# Adjusted Logistic Regression Analysis including confounders (gender and fasting blood sugar)

```{r}
adjusted_model <- glm(target_binary ~ elevated_BP * elevated_oldpeak + gender + fastingbloodsugar, 
                      data = cleaned_df, family = binomial)
summary(adjusted_model)

# Extract Odds Ratios (OR) for the adjusted model
adj_or_values <- exp(coef(adjusted_model))
adj_or_ci <- exp(confint(adjusted_model))
adj_or_results <- data.frame(
  Term = names(adj_or_values),
  OR = adj_or_values,
  CI_lower = adj_or_ci[, 1],
  CI_upper = adj_or_ci[, 2]
)
print("Adjusted Odds Ratios (with 95% CI):")
print(adj_or_results)

# Tidy the adjusted model output and compute odds ratios and 95% CI
tidy_adjusted_model <- tidy(adjusted_model, exponentiate = TRUE, conf.int = TRUE)
print(tidy_adjusted_model)
```

The adjusted logistic regression model produces unstable estimates. The intercept's odds ratio is about 0.07, indicating very low baseline odds. The "elevated_BPOtherwise" predictor has an OR of roughly 2.51, suggesting a potential increase in risk, but its effect is not statistically significant. The effect for "elevated_oldpeakNormal" is extremely low and unreliable, and both the gender and fasting blood sugar estimates are unusually large with wide confidence intervals. These findings indicate that the model may be suffering from separation or sparse data issues, rendering the parameter estimates unstable. This interpretation is derived solely from your output.

# PART 4b: MODEL BUILDING

```{r}
# Set seed for reproducibility
rdmseed <- 123

# Reset seed before modeling
set.seed(rdmseed)

cleaned_df <- cleaned_df %>%
  mutate(target_binary = if_else(target == "Heart Disease", 1, 0))
```

# linear model

```{r}
# Fit a standard linear regression model using predictors
lm_model <- linear_reg() %>%
  fit(target_binary ~ elevated_BP + elevated_oldpeak, data = cleaned_df)

# Get predictions from linear model and calculate RMSE
cleaned_df$pred_lm <- predict(lm_model, new_data = cleaned_df)$.pred
rmse(cleaned_df, truth = target_binary, estimate = pred_lm)

# # Plot observed vs. predicted for Linear Model
ggplot(cleaned_df, aes(x = target_binary, y = pred_lm)) +
  geom_point(color = "darkblue", alpha = 0.7, size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", size = 1) +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1))

```

The linear model yields an RMSE of about 0.47, suggesting that its predictions deviate from the true 0/1 outcome by nearly half on average. This indicates limited accuracy for distinguishing between individuals with and without heart disease.

# LASSO

```{r}
set.seed(rdmseed)

# Prepare recipe for LASSO with normalization
rec <- recipe(target_binary ~ elevated_BP + elevated_oldpeak, data = cleaned_df) %>%
  step_dummy(elevated_BP, elevated_oldpeak) %>%   # Convert categorical predictors to dummies
  step_normalize(all_numeric(), -all_outcomes())


# Define LASSO model with a fixed penalty
lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet")

# Create a workflow and fit the LASSO model
wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lasso_spec)

lasso_fit <- fit(wf, data = cleaned_df)
cleaned_df$pred_lasso <- predict(lasso_fit, new_data = cleaned_df)$.pred
rmse(cleaned_df, truth = target_binary, estimate = pred_lasso)


# Plot observed vs. predicted for LASSO Model
ggplot(cleaned_df, aes(x = target_binary, y = pred_lasso)) +
  geom_point(color = "darkviolet", alpha = 0.7, size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", size = 1) +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1))
```

The LASSO model shows an RMSE of about 0.48, indicating that its predictions deviate from the actual 0/1 outcome by nearly half on average. This suggests that it provides limited predictive accuracy for identifying heart disease status in this subset.

# Random Forest

```{r}
set.seed(rdmseed)

# Create a workflow and fit the Random Forest model
rf_model <- rand_forest() %>%
  set_mode("regression") %>%
  set_engine("ranger", seed = rdmseed)

rf_fit <- rf_model %>%
  fit(target_binary ~ elevated_BP + elevated_oldpeak, data = cleaned_df)

# Get predictions and evaluate model performance
cleaned_df$pred_rf <- predict(rf_fit, cleaned_df)$.pred
rmse(cleaned_df, truth = target_binary, estimate = pred_rf)


# Plot observed vs. predicted for RF Model
ggplot(cleaned_df, aes(x = target_binary, y = pred_rf)) +
  geom_point(color = "darkgreen", alpha = 0.7, size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red", size = 1) +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1))

```

The random forest model produces an RMSE of about 0.47, indicating that, on average, its predicted probabilities deviate from the true 0/1 outcome by nearly half. This suggests limited accuracy in distinguishing between individuals with and without heart disease in this dataset.

# Tuning the LASSO model (without cross-validation)

```{r}
set.seed(rdmseed)

# Create a grid of penalty values for LASSO
lasso_grid <- tibble(penalty = 10^seq(-5, 2, length.out = 50))

# Define LASSO model with tunable penalty
lasso_spec_tune <- linear_reg(penalty = tune()) %>%
  set_engine("glmnet")

# Create workflow and assign to lasso_tuning_workflow (new name)
lasso_tuning_workflow <- workflow() %>%
  add_model(lasso_spec_tune) %>%
  add_recipe(rec)

# Perform grid search without CV (apparent resampling)
lasso_tune <- tune_grid(
  lasso_tuning_workflow,
  resamples = apparent(cleaned_df),
  grid = lasso_grid,
  metrics = metric_set(rmse)
)

# Extract tuning results
lasso_tune_df <- as.data.frame(lasso_tune$.metrics)

# Plot LASSO tuning results
ggplot(lasso_tune_df, aes(x = penalty, y = .estimate)) +
  geom_line(size = 1, color = "steelblue") +
  geom_point(size = 2, color = "firebrick") +
  scale_x_log10() +
  labs(
    x = "Penalty (log scale)",
    y = "RMSE",
    title = "LASSO Tuning (No Cross-validation)",
    subtitle = "Using apparent resampling"
  ) +
  theme_minimal()
```

The tuning curve shows that the RMSE stays around 0.474 at lower penalty values, then increases to roughly 0.48 at higher penalties. This pattern suggests that a moderate level of regularization gives the best performance, while heavier penalties lead to worse predictive accuracy.

# Tuning for our Random Forest model (without cross-validation)

```{r}
# Create tuning grid for mtry and min_n
rf_grid <- grid_regular(
  mtry(range = c(1, 7)),
  min_n(range = c(1, 21)),
  levels = 7
)

# Define random forest model with tunable parameters
rf_spec_tune <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 300
) %>%
  set_mode("regression") %>%
  set_engine("ranger", seed = rdmseed)

# Build workflow
rf_wf_tune <- workflow() %>%
  add_model(rf_spec_tune) %>%
  add_recipe(rec)

# Perform RF tuning without CV
rf_tune <- tune_grid(
  rf_wf_tune,
  resamples = apparent(cleaned_df),
  grid = rf_grid,
  metrics = metric_set(rmse)
)

rf_tune_df <- as.data.frame(rf_tune$.metrics)

# Plot RF tuning results
ggplot(rf_tune_df, aes(x = mtry, y = min_n, fill = .estimate)) +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(
    title = "Random Forest Tuning (No Cross-validation)",
    x = "mtry",
    y = "min_n",
    fill = "RMSE"
  ) +
  theme_minimal()
```

The RMSE values for different combinations of mtry and min_n appear to hover around 0.4737 to 0.4739. This narrow range suggests that the model’s performance does not vary dramatically across these hyperparameter settings when no cross-validation is used. In other words, none of the parameter combinations stands out as substantially better or worse, indicating that random forest performance remains relatively stable within this grid.

# Cross-validation for our LASSO model tuning

```{r}
set.seed(rdmseed)

# Create 5-fold CV with 5 repeats
cv_folds <- vfold_cv(cleaned_df, v = 5, repeats = 5)


# Tune LASSO using CV
lasso_tune_cv <- tune_grid(
  lasso_tuning_workflow,
  resamples = cv_folds,
  grid = lasso_grid,
  metrics = metric_set(rmse)
)

autoplot(lasso_tune_cv)
```

Based on the chart, the RMSE begins at roughly 0.497 when the regularization amount is very low, then drops to around 0.491 and stays there as the regularization increases. This suggests that moderate to higher levels of regularization lead to better performance, while minimal regularization yields higher error.

# Cross-validation tuning for our random forest model

```{r}
# Tune RF using CV
rf_tune_cv <- tune_grid(
  rf_wf_tune,
  resamples = cv_folds,
  grid = rf_grid,
  metrics = metric_set(rmse)
)
autoplot(rf_tune_cv)

```

Based on the chart, the pink line (representing a minimal node size of 21) shows the lowest RMSE across all numbers of randomly selected predictors (mtry). In contrast, the other lines, which represent smaller minimal node sizes, produce higher RMSE values. This pattern suggests that using a larger minimal node size leads to better performance in this grid, while varying mtry has a smaller impact on the overall error.
