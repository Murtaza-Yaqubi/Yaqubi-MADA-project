---
title: "Model Tuning"
author: "Murtaza Yaqubi"
format: html
editor: visual
---

```{r}
# Tuning the LASSO model (without cross-validation)
set.seed(rdmseed)

# Create a grid of penalty values for LASSO
lasso_grid <- tibble(penalty = 10^seq(-5, 2, length.out = 50))

# Define LASSO model with tunable penalty
lasso_spec_tune <- linear_reg(penalty = tune()) %>%
  set_engine("glmnet")

# Create workflow and assign to lasso_tuning_workflow (new name)
lasso_tuning_workflow <- workflow() %>%
  add_model(lasso_spec_tune) %>%
  add_recipe(rec)

# Perform grid search without CV (apparent resampling)
lasso_tune <- tune_grid(
  lasso_tuning_workflow,
  resamples = apparent(cleaned_df),
  grid = lasso_grid,
  metrics = metric_set(rmse)
)

# Extract tuning results
lasso_tune_df <- as.data.frame(lasso_tune$.metrics)

# Identify best LASSO penalty
best_lasso <- lasso_tune_df %>% filter(.estimate == min(.estimate))
print("Best LASSO Penalty (No CV):")
print(best_lasso)

# Plot LASSO tuning results
ggplot(lasso_tune_df, aes(x = penalty, y = .estimate)) +
  geom_line(size = 1, color = "steelblue") +
  geom_point(size = 2, color = "firebrick") +
  scale_x_log10() +
  labs(
    x = "Penalty (log scale)",
    y = "RMSE",
    title = "LASSO Tuning (No Cross-validation)",
    subtitle = "Using apparent resampling"
  ) +
  theme_minimal()

# Save LASSO tuning plot
lasso_tune_plot <- last_plot()
ggsave(filename = here("results", "figures", "figure2_lasso_tuning_plot.png"), plot = lasso_tune_plot)

# Tuning for our Random Forest model (without cross-validation)
rf_grid <- grid_regular(
  mtry(range = c(1, 7)),
  min_n(range = c(1, 21)),
  levels = 7
)

# Define random forest model with tunable parameters
rf_spec_tune <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 300
) %>%
  set_mode("regression") %>%
  set_engine("ranger", seed = rdmseed)

# Build workflow
rf_wf_tune <- workflow() %>%
  add_model(rf_spec_tune) %>%
  add_recipe(rec)

# Perform RF tuning without CV
rf_tune <- tune_grid(
  rf_wf_tune,
  resamples = apparent(cleaned_df),
  grid = rf_grid,
  metrics = metric_set(rmse)
)

rf_tune_df <- as.data.frame(rf_tune$.metrics)

# Identify best RF tuning parameters
best_rf <- rf_tune_df %>% filter(.estimate == min(.estimate))
print("Best Random Forest Parameters (No CV):")
print(best_rf)

# Plot RF tuning results
ggplot(rf_tune_df, aes(x = mtry, y = min_n, fill = .estimate)) +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(
    title = "Random Forest Tuning (No Cross-validation)",
    x = "mtry",
    y = "min_n",
    fill = "RMSE"
  ) +
  theme_minimal()

# Save Random Forest tuning heatmap
rf_tune_plot <- last_plot()
ggsave(filename = here("results", "figures", "figure3_rf_tuning_heatmap.png"), plot = rf_tune_plot)

# Cross-validation for our LASSO model tuning
set.seed(rdmseed)

# Create 5-fold CV with 5 repeats
cv_folds <- vfold_cv(cleaned_df, v = 5, repeats = 5)

# Tune LASSO using CV
lasso_tune_cv <- tune_grid(
  lasso_tuning_workflow,
  resamples = cv_folds,
  grid = lasso_grid,
  metrics = metric_set(rmse)
)

autoplot(lasso_tune_cv)

# Cross-validation tuning for our random forest model
rf_tune_cv <- tune_grid(
  rf_wf_tune,
  resamples = cv_folds,
  grid = rf_grid,
  metrics = metric_set(rmse)
)

autoplot(rf_tune_cv)
```